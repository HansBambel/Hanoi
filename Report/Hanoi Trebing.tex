\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{multicol}

\title{Foundations of Agents: Practical Assignment 1}
\author{Kevin Trebing (i6192338)}

\begin{document}
\maketitle

\section{Tower of Hanoi problem}
In a Tower of Hanoi problem the agent needs to move disks of different sizes from one pin to another pin. Furthermore, the disks need to be in the order that a smaller disk needs to be on top of a bigger one. Only one disk can be moved at a time and only the topmost disk can be moved. In our problem we have 3 pins and two disks. The starting position is pin 1 and the smaller disk is on the bigger disk. The goal is to move the disks to pin 3.

\subsection{Description of States}
Notation:
\begin{multicols}{2}
\begin{itemize}

\item a = small disk,
\item b = big disk,
\item 1 = pin1,
\item 2 = pin2,
\item 3 = pin3,
\item ab = a is on b
\end{itemize}
\end{multicols}

\noindent
We have 12 different possible states:
\begin{multicols}{2}
\begin{itemize}
\item $s_{0}$: ab1 2 3
\item $s_{1}$: 1 ab2 3
\item $s_{2}$: 1 2 ab3
\item $s_{3}$: ba1 2 3
\item $s_{4}$: 1 ba2 3
\item $s_{5}$: 1 2 ba3
\item $s_{6}$: b1 a2 3
\item $s_{7}$: a1 b2 3
\item $s_{8}$: b1 2 a3
\item $s_{9}$: a1 2 b3
\item $s_{10}$: 1 a2 b3
\item $s_{11}$: 1 b2 a3
\end{itemize}
\end{multicols}

\subsection{Description of Actions}
We have 6 different actions that the agent can take.
\begin{multicols}{2}
\begin{itemize}
\item $a_{1}$: move a to pin1
\item $a_{2}$: move a to pin2
\item $a_{3}$: move a to pin3
\item $b_{1}$: move b to pin1
\item $b_{2}$: move b to pin2
\item $b_{3}$: move b to pin3
\end{itemize}
\end{multicols}

\section{Optimal policy}
The optimal policy describes for every state the best action the agent should take.

\begin{multicols}{2}
\begin{itemize}
\item $\pi(s_{0}) = a_{2}$
\item $\pi(s_{1}) = a_{1}$
\item $\pi(s_{2}) = a_{2}$
\item $\pi(s_{3}) = b_{3}$
\item $\pi(s_{4}) = b_{3}$
\item $\pi(s_{5}) = b_{1}$ according to PI and $b_{2}$ according to VI
\item $\pi(s_{6}) = b_{3}$
\item $\pi(s_{7}) = b_{3}$
\item $\pi(s_{8}) = a_{2}$
\item $\pi(s_{9}) = a_{3}$
\item $\pi(s_{10}) = a_{3}$
\item $\pi(s_{11}) = a_{1}$
\end{itemize}
\end{multicols}

\section{Utility}
The utility of each state given the optimal policy. The utility is calculated the following:
\begin{equation}
u(s,\pi) = r(s,\pi(s)) + \gamma \cdot \sum_{s' \epsilon S} t(s,\pi(s),s') \cdot u(s',\pi)
\end{equation}
Using the values of the policy-iteration:
\begin{multicols}{2}
\begin{itemize}
\item $u(s_0) = 73.59 $
\item $u(s_1) = 62.27 $
\item $u(s_2) = 0.00 $
\item $u(s_3) = 86.00 $
\item $u(s_4) = 86.63 $
\item $u(s_5) = 53.27 $
\item $u(s_6) = 85.91 $
\item $u(s_7) = 85.81 $
\item $u(s_8) = 74.31 $
\item $u(s_9) = 98.79 $
\item $u(s_{10}) = 98.79 $
\item $u(s_{11}) = 74.11 $
\end{itemize}
\end{multicols}

\noindent
Using value-iteration the following utility-values arise:
\begin{multicols}{2}
\begin{itemize}
\item $u(s_0) = 75.31 $
\item $u(s_1) = 75.39 $
\item $u(s_2) = 0.00 $
\item $u(s_3) = 86.75 $
\item $u(s_4) = 86.75 $
\item $u(s_5) = 66.77 $
\item $u(s_6) = 85.93 $
\item $u(s_7) = 85.93 $
\item $u(s_8) = 74.48 $
\item $u(s_9) = 98.79 $
\item $u(s_{10}) = 98.79 $
\item $u(s_{11}) = 75.39 $
\end{itemize}
\end{multicols}

\section{Conclusion}
The policies for value-iteration and policy-iteration differed in one state ($s_5$). But apart from that they were similar. Interestingly, the policy-iteration was significantly faster than value-iteration: value-iteration needed about 0.16s to converge whereas policy-iteration only needed 0.0033s to converge. Furthermore, policy-iteration delivers a more accurate result.

\end{document}
