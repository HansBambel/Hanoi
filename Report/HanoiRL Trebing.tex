\documentclass[a4paper]{article}
\usepackage{amsmath}
\usepackage{multicol}

\title{Foundations of Agents: Practical Assignment 1}
\author{Kevin Trebing (i6192338)}

\begin{document}
\maketitle

\section{Tower of Hanoi problem}
In a Tower of Hanoi problem the agent needs to move disks of different sizes from one pin to another pin. Furthermore, the disks need to be in the order that a smaller disk needs to be on top of a bigger one. Only one disk can be moved at a time and only the topmost disk can be moved. In our problem we have 3 pins and two disks. The starting position is pin 1 and the smaller disk is on the bigger disk. The goal is to move the disks to pin 3.

\subsection{Description of States}
Notation:
\begin{multicols}{2}
\begin{itemize}

\item a = small disk,
\item b = big disk,
\item 1 = pin1,
\item 2 = pin2,
\item 3 = pin3,
\item ab = a is on b
\end{itemize}
\end{multicols}

\noindent
We have 12 different possible states: \\

\begin{tabular}{ c|c|c|c}
	State & \multicolumn{3}{c}{Pin} \\
	\hline
	$s_{0}$ & ab1 & 2 & 3 \\
	$s_{1}$ & 1 & ab2 & 3 \\
	$s_{2}$ & 1 & 2 & ab3 \\
	$s_{3}$ & ba1 & 2 & 3 \\
	$s_{4}$ & 1 & ba2 & 3 \\
	$s_{5}$ & 1 & 2 & ba3 \\
	$s_{6}$ & b1 & a2 & 3 \\
	$s_{7}$ & a1 & b2 & 3 \\
	$s_{8}$ & b1 & 2 & a3 \\
	$s_{9}$ & a1 & 2 & b3 \\
	$s_{10}$ & 1 & a2 & b3 \\
	$s_{11}$ & 1 & b2 & a3

\end{tabular}

\newpage
\subsection{Description of Actions}
We have 6 different actions that the agent can take. \\

\begin{tabular}{c|c}
Action & effect \\
\hline
$a_{1}$ & move a to pin1 \\
$a_{2}$ & move a to pin2 \\
$a_{3}$ & move a to pin3 \\
$b_{1}$ & move b to pin1 \\
$b_{2}$ & move b to pin2 \\
$b_{3}$ & move b to pin3
\end{tabular}

\section{How the agent learns the optimal result for every initial state}
Every time the agent ends in the absorbing state the new state is randomly chosen. Given enough resets the agent will start in every state enough times to approximate the optimal result. In my implementation I do 10000 iterations which results in every state being the starting state about 833 times.

\section{Optimal policy}
The optimal policy describes for every state the best action the agent should take.

\begin{multicols}{2}
\begin{itemize}
\item $\pi(s_{0}) = a_{2}$
\item $\pi(s_{1}) = a_{1}$
\item $\pi(s_{2}) = a_{1}$
\item $\pi(s_{3}) = b_{3}$
\item $\pi(s_{4}) = b_{3}$
\item $\pi(s_{5}) = b_{1}$
\item $\pi(s_{6}) = b_{3}$
\item $\pi(s_{7}) = b_{3}$
\item $\pi(s_{8}) = a_{2}$
\item $\pi(s_{9}) = a_{3}$
\item $\pi(s_{10}) = a_{3}$
\item $\pi(s_{11}) = a_{1}$
\end{itemize}
\end{multicols}

\section{Q-values}

The q-values of of the states given the optimal policy:
\begin{multicols}{2}
\begin{itemize}
\item $q(s_0) = 75.30 $
\item $q(s_1) = 75.26 $
\item $q(s_2) = 0.00 $
\item $q(s_3) = 87.11 $
\item $q(s_4) = 86.98 $
\item $q(s_5) = 66.62 $
\item $q(s_6) = 85.63 $
\item $q(s_7) = 85.99 $
\item $q(s_8) = 74.99 $
\item $q(s_9) = 98.54 $
\item $q(s_{10}) = 98.83 $
\item $q(s_{11}) = 74.96 $
\end{itemize}
\end{multicols}

\section{Difference to value and policy iteration}
Convergence depended on the learning rate a lot. At first I used $\frac{1}{numVisits}$, but this resulted in bad results. The reason for this is that the initial probability of choosing an action is the same for every action, but as soon as the first q-value for that state is updated with an initial learning rate of 1 the probability to choose this action is way higher than the other actions. 

When fixing the learning rate at $0.01$ the probability of choosing an action does not change very much, this is what we want in the beginning of learning to encourage exploration. A small learning rate stabilizes learning.

The q-values are about the same as the utility values calculated in the previous assignment.

\section{Convergence speed}
Since I have a fixed learning rate of $0.01$ it needs a lot of loops to converge to optimal values (optimal values are the values that were calculated by policy iteration in the earlier assignment). Using 10000 iterations I get very similar q-values compared to the optimal utility values. The time needed for 10000 iterations is about 2.5s.

\section{Note}
The Hanoi.py file requires Python 3.6.

\end{document}
